{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9035a5d-7a6b-4b2f-b027-375dbae8c40e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 1: Install Dependencies\n",
    "First, make sure you have MLflow and scikit-learn installed. You can install them using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71d71aac-6707-4361-9d2c-6a7b272b3213",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install mlflow scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f16f265-bc4a-486d-9257-85fab556e4f1",
   "metadata": {},
   "source": [
    "### Step 2: Import Libraries\n",
    "In your Python script, import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fcbde98-775d-477b-a2cd-a4d932c87032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc34b709-0a4d-461f-ad7f-dfd51e207c0f",
   "metadata": {},
   "source": [
    "### Step 3: Load and Prepare Data\n",
    "Load a dataset from scikit-learn, split it into training and testing sets, and perform any necessary data preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de1041eb-81df-4e8b-b195-c8162e299f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_diabetes()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307bd01c-bb94-472d-acbd-14476608f43c",
   "metadata": {},
   "source": [
    "### Step 4: Train and Log the Model\n",
    "Train a simple linear regression model using scikit-learn and log it using MLflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66310822-7bd4-407f-ae39-d8918032e431",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ramin/anaconda3/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    # Create and train a linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(model, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74e4ab1-1738-4730-9e35-1f694fdcbd0c",
   "metadata": {},
   "source": [
    "This code initializes an MLflow run, trains the model, and logs it with the name \"model.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded35bdd-3f3a-4f74-a5f6-464fd075a93a",
   "metadata": {},
   "source": [
    "### Step 5: Save the MLflow Artifacts\n",
    "By default, MLflow logs the model into an artifact store. You can specify the artifact location in your MLflow server configuration or use the default location. You can also set the environment variable MLFLOW_TRACKING_URI to point to your MLflow server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d6225d-22cf-4357-91ba-be31f85a1de9",
   "metadata": {},
   "source": [
    "### Step 6: Serve the Model\n",
    "To serve the model using MLflow, you can use the mlflow models serve command. First, make sure that MLflow is running as a server (e.g., mlflow server).\n",
    "\n",
    "Then, you can start serving your model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1886de3c-cb96-42ec-9adb-77e9ad8615e2",
   "metadata": {},
   "source": [
    "### In case of Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8d1fc4c-976b-4cf9-8fe7-9110739de0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pyenv/pyenv/wiki#suggested-build-environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64bbdb26-27a7-4247-9d10-8e7d9f114dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf '/home/ramin/.pyenv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "910d7e81-336f-4e36-9622-75e84fe5edfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl https://pyenv.run | bash\n",
    "# !python -m  pip install virtualenv\n",
    "# !PATH=\"$HOME/.pyenv/bin:$PATH\"\n",
    "# !export PYENV_ROOT=\"$HOME/.pyenv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c512afc-9065-420c-b020-3a2b67397286",
   "metadata": {},
   "source": [
    "### In-case of dependeny error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe3d4386-6672-416d-a039-72176c64c728",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/27 20:59:18 INFO mlflow.pyfunc: To install the dependencies that were used to train the model, run the following command: '%pip install -r /home/ramin/Documents/Teaching/IE7374-MLOPS/Labs/Mlflow_Labs/mlruns/0/b2b3ad10af1a4fc7a8387f810251093e/artifacts/model/requirements.txt'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/ramin/Documents/Teaching/IE7374-MLOPS/Labs/Mlflow_Labs/mlruns/0/b2b3ad10af1a4fc7a8387f810251093e/artifacts/model/requirements.txt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mlflow.pyfunc.get_model_dependencies('mlruns/0/b2b3ad10af1a4fc7a8387f810251093e/artifacts/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbfa79a-7faf-40f6-adf0-137996637354",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip3 install -r mlruns/0/aea73ab36d544af29927cf26199b8888/artifacts/model/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148adc83-a920-4d11-8e74-8dfd4c900641",
   "metadata": {},
   "source": [
    "### To run on a severate bash screen within the same venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8995c1-a3a6-469e-9302-712cf142bd54",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mlflow models serve --env-manager=local -m mlruns/0/b2b3ad10af1a4fc7a8387f810251093e/artifacts/model -h 127.0.0.1 -p 5001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e098e63-92fc-4e70-aadd-8780ba9cf18e",
   "metadata": {},
   "source": [
    "### Step 7: Make Predictions\n",
    "You can make predictions by sending HTTP POST requests to the model server. Here's an example using Python's requests library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2319aa69-4b0c-4c18-b8d0-358fff9990a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96f162c7-f96e-48dd-b5b6-932dbaed9b81",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [139.547558403796, 179.5172083534279, 134.03875571890103, 291.4170292522083, 123.78965872239598, 92.17234650105036, 258.232388989213, 181.33732057060715, 90.22411310941453, 108.63375858007923, 94.13865743638848, 168.43486357975485, 53.504788796095596, 206.6308165929642, 100.12925868961597, 130.66657085052714, 219.53071499182573, 250.7803234033336, 196.36883460186402, 218.57511814821788, 207.35050181974754, 88.4834094142032, 70.43285917378932, 188.95914235039007, 154.886816196898, 159.36170121791906, 188.31263362697564, 180.39094032857133, 47.99046560870016, 108.97453871427206, 174.77897632897384, 86.36406655640143, 132.9576121467635, 184.53819482628643, 173.83220911339018, 190.35858491915604, 124.41561760139213, 119.6511065607128, 147.95168682418483, 59.05405241472812, 71.62331856151582, 107.68284703885509, 165.45365458042076, 155.00975931051346, 171.0479909563818, 61.457613561017894, 71.66672580728223, 114.96732205941949, 51.579755230423885, 167.57599528011522, 152.52291954830028, 62.95568515225264, 103.49741722264709, 109.20751488540836, 175.6411842576666, 154.6029624181131, 94.41704365715881, 210.74209144739245, 120.25662049594234, 77.61585398627129, 187.9320399528739, 206.49337473788225, 140.63167075673582, 105.59678022893402, 130.70432535999663, 202.1853453667185, 171.1303950081213, 164.91423047268822, 124.72472568780734, 144.81030894117254, 181.99635451907636, 199.41369641711736, 234.21436187590186, 145.95665512205622, 79.86703276235616, 157.36941274964417, 192.74412541087804, 208.89814031629626, 158.5872255494477, 206.02195855372946, 107.4797167461347, 140.93598906417054, 54.82129331926056, 55.9257319471196, 115.01180017858908, 78.9558418752068, 81.56087284620637, 54.37997255644882, 166.25435180326872]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = 'http://127.0.0.1:5001/invocations'\n",
    "\n",
    "data = {\n",
    "    \"columns\": data['feature_names'],\n",
    "    \"instances\": X_test.tolist()\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "predictions = response.json()\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9376ff-b1fd-4b65-ad59-f6c302d06d95",
   "metadata": {},
   "source": [
    "The url variable, in the context of the code provided, represents the Uniform Resource Locator (URL) that is used to specify the location or address of a web resource. In this case, it points to a specific endpoint on a local web server:\n",
    "\n",
    "* http:// indicates the protocol being used, which is Hypertext Transfer Protocol (HTTP). HTTP is the foundation of data communication on the World Wide Web.\n",
    "\n",
    "* localhost is a special hostname that typically refers to the current device or computer where the code is running. It is often used to access services running on the same machine.\n",
    "\n",
    "* 5000 is the port number. Ports are used to differentiate between different services or processes running on the same machine. Port 5000 is a commonly used port for running web servers locally.\n",
    "\n",
    "* /invocations is the path or endpoint on the web server. In the context of machine learning deployment, this path often represents an API endpoint for making predictions using a deployed machine learning model. In this specific case, it's likely that the web server running on localhost:5000 has an endpoint called /invocations that allows you to send data to the deployed model and receive predictions in response.\n",
    "\n",
    "So, the url variable is essentially storing the URL of the API endpoint where you can send a POST request with data to obtain predictions from a locally deployed machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243ec584-d382-4fee-90fa-b69b4b76e3b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
